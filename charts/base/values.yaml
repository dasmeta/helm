# Default values for datalertio.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is  the chart appVersion.

# Init containers
# initContainers:
#   - name: config
#     args:
#     - /config.json
#     - /test/config.json
#     secrets:
#       - CLIENT_ID:
#           from: client-market
#           key: client_id
#       - AUTH_TOKEN:
#           from: auth-token
#           key: token
#     image:
#       repository: nginx
#       pullPolicy: IfNotPresent
#       tag: latest
#     volumes:
#       - mountPath: /config.json
#         name: config-json
#         subPath: config.json

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
selectorLabelsOverride: {} # allows to override selector labels, this can be used for example in case when we want to have deployment pods attached to another release service(see in repo folder /examples/base/with-second-deployment)

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

job:
  labels:
    - name: app.kubernetes.io/component
      value: job

  serviceAccount:
    create: false
    annotations: {}

podAnnotations: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
# runAsUser: 1000

service:
  enabled: true
  name: http
  type: NodePort
  port: 80
  protocol: TCP
  extraPorts: []
  ## here is sample how you can set/enable additional ports on service
  # extraPorts:
  #   - port: 8080
  #     targetPort: 8080
  #     protocol: TCP
  #     name: http-second

# The ingress class setting by annotation is deprecated and we have setIngressClassByField option for backward compatibility for now, check doc here https://kubernetes.io/docs/concepts/services-networking/ingress/#deprecated-annotation
# have 'setIngressClassByField: true' to set ingress class by spec.ingressClassName,
# we seems cant have both "kubernetes.io/ingress.class" and spec.ingressClassName set at once, so there is check(for alb, application-gateway and cce the check is missing for now) to not set annotation if we set ingress class fy field
# TODO: have ingress class setting by field used always and remove deprecated "kubernetes.io/ingress.class" annotations
setIngressClassByField: false

# it allows also to pass list of ingresses in following form
# NOTE: in case of list it will auto attach uniq identify numbers to ingress names starting from second item, you can use nameSuffix field to set custom suffix instead of number
ingress:
  enabled: false
  class: alb
  annotations: {}
  hosts:
    - host: host.name.com
      paths:
        - path: /*
          backend:
            serviceName: ssl-redirect
            servicePort: use-annotation
        - backend:
            serviceName: base
            servicePort: 80
          path: /*
  tls: []
# example how can be ingress list passed
# ingress:
#   - class: nginx
#     hosts:
#       - host: host.name.com
#         paths:
#           - backend:
#               serviceName: base
#               servicePort: 80
#             path: /*
#   - class: nginx-second
#     nameSuffix: -second
#     hosts:
#       - host: host-second.name.com
#         paths:
#           - path: /*
#             backend:
#               serviceName: base
#               servicePort: 8080

secrets: []
secretsDefaultEngine: "ExternalSecrets"
externalSecretsApiVersion: external-secrets.io/v1alpha1 # TODO: the new version external-secrets.io/v1beta1 is available in external-secret operator, please update to new version as soon as you upgrade operator(the new dasmeta eks module already uses the new one)
# secrets:
#   - external-secret-1
#   - external-secret-2
#   - external-secret-3
#   - external-secret-4
#   - DATABASE:
#       from: helm-test-secret
#       key: database
#   - ENDPOINT:
#       from: helm-test-secret
#       key: endpoint
#   - PASSWORD:
#       from: helm-test-secret
#       key: password
#   - SPRING_DATASOURCE_URL:
#       from: helm-test-secret
#       key: spring_datasource_url
#   - USERNAME:
#       from: helm-test-secret
#       key: username

extraContainer: {} # allows to have additional containers along side to main one, both single object(if you have just one extra) and list of objects are supported
# extraContainer:
#   - name: extra-nginx
#     image:
#       repository: nginx
#       tag: 1.19
#     extraEnv:
#       NGINX_PORT: 8081
#   - name: extra-second-nginx
#     containerPort: 8082
#     service:
#       enabled: true # to have container port definition in deployment
#     image:
#       repository: nginx
#       tag: 1.19
#     extraEnv:
#       NGINX_PORT: 8082
#   - name: extra-third-nginx
#     containerPort: 8083
#     # allows to have extra ports on extra container. generally there is no need for this in order to use it in service as target, but some tools, like prometheus podMonitor/serviceMonitor need port configs set on pod
#     extraPorts:
#       - name: http-third-second
#         containerPort: 8084
#         protocol: TCP
#     service:
#       enabled: true # to have container port definition in deployment
#     image:
#       repository: nginx
#       tag: 1.19
#     extraEnv:
#       NGINX_PORT: 8083

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
#   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

# If you want scale deployment depends on AWS SQS queue size
# autoscaling:
#   enabled: true
#   minReplicas: 1
#   maxReplicas: 3
#   targetCPUUtilizationPercentage: 80
#   trigger:
#     type: aws-sqs-queue
#     metadata:
#       queueURL: https://sqs.eu-central-1.amazonaws.com/<account-id>/<sqs-name>
#       queueLength: 3
#       awsRegion: "eu-central-1"

nodeSelector: {}

tolerations: []

affinity: {}

config: {}

containerPort: 80
## allows to have extra ports on main container. generally there is no need for this in order to use it in service as target, but some tools, like prometheus podMonitor/serviceMonitor need port configs set on pod
# extraPorts:
#   - name: http-second
#     containerPort: 8080
#     protocol: TCP

livenessProbe:
  {}
  # failureThreshold: 3
  # httpGet:
  #   path: /
  #   port: 80
  #   scheme: HTTP
  # initialDelaySeconds: 60
  # periodSeconds: 5
  # successThreshold: 1
# timeoutSeconds: 1

readinessProbe:
  {}
  # httpGet:
  #   path: /
  #   port: http
  # initialDelaySeconds: 60
# periodSeconds: 5

envFrom:
  secret:

startupProbe:
  {}
  # httpGet:
  #   path: /
#   port: http

storage:
  {}
  # - persistentVolumeClaimName: "pvc-1"
  #   accessModes:
  #     - ReadWriteMany
  #   className: efs-sc
  #   requestedSize: 2Gi
  #   enableDataSource: false
  # - persistentVolumeClaimName: pvc-2
  #   accessModes:
  #     - ReadWriteMany
  #   className: efs-sc-root
  #   requestedSize: "2Gi"
  #   enableDataSource: false

deployment: {} # option is deprecated and all underlying fields are now available as top level variables, so that `deployment.volumes` changed to `volumes`, `deployment.additionalvolumeMounts` to `additionalVolumeMounts` and `deployment.lifecycle` to `lifecycle`
volumes: []
# volumes:
#   - emptyDir: # if no container field passed like this case it will be mounted to main container
#       medium: Memory
#       sizeLimit: 10Mi
#     name: volume-only-main-container
#     mountPath: /volume-main
#   - emptyDir:
#       medium: Memory
#       sizeLimit: 10Mi
#     name: volume-only-extra-container
#     mountPath: /volume-extra
#     container: extra-container-name
#   - emptyDir:
#       medium: Memory
#       sizeLimit: 10Mi
#     name: volume-main-and-extra-containers
#     mountPath: /volume-shared
#     container: [main-container-name, extra-container-name]
#   - configMap:
#       name: config-map-name
#     name: volume-config-map
#     mountPath: /volume-config-map/config-map.txt
#     subPath: config-map.txt
#   - secret:
#       items:
#         - key: release
#           path: data.txt
#       secretName: secret-name
#     name: volume-secret
#     mountPath: /volume-secret/
#   - persistentVolumeClaim:
#       claimName: pvc-name
#     mountPath: /volume-pvc/
#     name: volume-pvc

env: dev
product: application

pdb:
  enabled: false
  minAvailable: 1
  pdbName:

# the default lifecycle rule to apply which allows to have some kind of graceful shutdown by waiting/sleep for 5 seconds, even if there is no sleep command in os this will not produce issues and container will get shuted down
defaultLifecycle:
  enabled: true
  preStop:
    exec:
      command:
        - sh
        - -c
        - sleep 5 # the sleep interval can vary for app, if you still get issues at pod termination consider increasing it little by little to get optimal one for you, based on tests the 5 seconds seems suitable for standard web app on eks to get the pod IP removed from service targets and to finish active client requests/connections to not have failed request at pod termination

serviceMonitor:
  enabled: false
  interval: 30s
  targetPort: 80
  path: /metrics
  # For authorization
  # authorization:
  #   credentials:
  #     key: TOKEN
  #     name: secret-name
  #   type: Token

# This config allows to enable custom rollout strategies by using different providers/operators
# right now only flagger (https://flagger.app/) operator supported and tested for canary with nginx
## NOTE for flagger operator:
## - flagger supports several service meshes and ingresses as provider for traffic splitting, and by default we have using nginx here, so you have to check docs and have at least one used for you app
## - you need to have flagger tool/operator already installed to be able to use its crd, this can be done by installing flagger helm https://artifacthub.io/packages/helm/flagger/flagger
## - also there is need to have at least one metric server/provider enabled(it supports) like prometheus as it uses metrics for checking success rates, the flagger helm allows to install prometheus
## - with flagger enabled we disable native kubernetes service as flagger creates/overrides this service
## - with separate installed prometheus operator(not one that comes with flagger helm) the default `request-success-rate` and `request-duration` metrics templates may not work so you may need to create custom metric templates, the canary+nginx+prometheus metric template can be created by using `dasmeta/flagger-metric-template` chart
rolloutStrategy:
  enabled: false
  operator: flagger
  configs: {}
  # here are all supported flagger configs
  # configs:
  #   # skipAnalysis: true # do not run analytics and immediately proceed to promotion as soon as we have canary pods running, can be used in case we need promote change without any webhook/metric checks (if canary pod still remain in pending/containerCreating state the immediate deploy will not work and canary may fail)
  #   provider: nginx # the flagger ingress/service-mesh provider (default nginx)
  #   progressDeadlineSeconds: 61 # the maximum time in seconds for the canary deployment to make progress before it is rollback (default 600s)
  #   canaryReadyThreshold: 51 # minimum percentage of canary pods that must be ready before considering canary ready for traffic shifting (default 100)
  #   primaryReadyThreshold: 51 # minimum percentage of primary pods that must be ready before considering primary ready for traffic shifting (default 100)
  #   interval: 11s # schedule interval (default 60s)
  #   threshold: 11 # max number of failed metric checks before rollback (default 10)
  #   maxWeight: 31 # max traffic percentage (0-100) routed to canary (default 30)
  #   stepWeight: 11 # canary increment step percentage (0-100) (default 10)
  #   # min and max replicas count for primary hpa, default to main app hpa, the main app hpa values also being used for canary deploy hpa so we use this options to have custom values for primary hpa
  #   primaryScalerMinReplicas: 3
  #   primaryScalerMaxReplicas: 7
  #   metrics: # metrics template configs to use for identifying if canary deploy handles request normally, the `request-success-rate` and `request-duration` named ones are available by default, and you can create custom metric templates
  #     - name: request-success-rate
  #       # minimum req success rate (non 5xx responses) percentage (0-100)
  #       thresholdRange:
  #         min: 99
  #       interval: 1m
  #     - name: request-duration
  #       # maximum req duration P99, milliseconds
  #       thresholdRange:
  #         max: 500
  #       interval: 1m
  #     # - name: request-success-rate-nginx-custom
  #     #   interval: 1m
  #     #   templateRef:
  #     #     name: request-success-rate-nginx-custom
  #     #     namespace: ingress-nginx
  #     #   # minimum req success rate (non 5xx responses) percentage (0-100)
  #     #   thresholdRange:
  #     #     min: 99
  #     # - name: request-duration-nginx-custom
  #     #   interval: 1m
  #     #   templateRef:
  #     #     name: request-duration-nginx-custom
  #     #     namespace: ingress-nginx
  #     #   # maximum req duration P99, milliseconds
  #     #   thresholdRange:
  #     #     max: 500
  #   webhooks: # (optional) webhooks can be used for load testing before traffic switching to canaries by using `pre-rollout` type and also generating traffic
  #     - name: acceptance-test
  #       type: pre-rollout
  #       url: http://flagger-loadtester.localhost/
  #       timeout: 30s
  #       metadata:
  #         type: bash
  #         cmd: "curl -sd 'test' http://http-echo-canary/ping | grep ping"
  #     - name: load-test
  #       url: http://flagger-loadtester.localhost/
  #       timeout: 5s
  #       metadata:
  #         cmd: "hey -z 1m -q 3 -c 1 http://http-echo.localhost/ping"
  #   alerts: # (optional) alerts allow to setup custom alerting/notify based on flaggers alert provider crd(there is also option to set global alert config with flagger), for more info look https://docs.flagger.app/usage/alerting
  #     - name: "on-call Slack"
  #       severity: error
  #       providerRef:
  #         name: on-call
  #         namespace: ingress-nginx
